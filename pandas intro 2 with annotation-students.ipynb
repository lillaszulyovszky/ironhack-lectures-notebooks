{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to using Pandas - with healthcare for all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is a Python library used for working with arrays.\n",
    "\n",
    "It also has functions for working in domain of linear algebra, fourier transform, and matrices.\n",
    "\n",
    "NumPy was created in 2005 by Travis Oliphant. It is an open source project and you can use it freely.\n",
    "\n",
    "NumPy stands for Numerical Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#step 1 import your libraries / packages - try running, if not installed use conda / pip to install \n",
    "# eg conda install -c anaconda numpy or pip install numpy \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#should be included with anaconda installation \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bring in the file and convert to a pandas dataframe  \n",
    "file1 = pd.read_csv('file1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the top rows of the first file using df.head()\n",
    "file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use describe to review whats in the columns and some basic descriptive statistics\n",
    "file1.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bring in the next file \n",
    "file2 = pd.read_csv('file2.txt',sep='\\t')\n",
    "# note the use of separators  necessary for txt files, not for csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the top rows of the second file using df.head()\n",
    "file2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the shape of the second file using df.shape\n",
    "file2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in and review the head/ shape of the remaining two excel files - this time using pd.read_excel, as file3 and file4\n",
    "\n",
    "file3=pd.read_excel('file3.xlsx')\n",
    "\n",
    "file3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4=pd.read_excel('file4.xlsx')\n",
    "file4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge the data frames \n",
    "\n",
    "after reviewing the column headers for a match we will combine data sources 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the column names for file 1 and 2 using df.columns \n",
    "file3.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pull out the column names from file1 as a variable (\n",
    "\n",
    "column_names=file1.columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this command will set those column names in the target data frame \n",
    "\n",
    "data=pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head() to review what we have \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets concatenate our new target data frame with our first file1\n",
    "data=pd.concat([data,file1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use head() to review what we have \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same again, concat the file 2 into the data df. \n",
    "data=pd.concat([data,file2],axis=0)\n",
    "\n",
    "#hint : be careful not to run this more than once or you will have to clear some output! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the shape to ensure you have the correct no of rows\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#before bringing in the other dfs lets confirm the data columns will line up for all files  \n",
    "\n",
    "data.columns\n",
    "\n",
    "#if they dont - what can be done? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if happy to proceed, lets concat in file 3 using the same method as before and review the head()\n",
    "\n",
    "data=pd.concat([data,file3],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets concat in file 4 using the same method as before and review the head()\n",
    "\n",
    "data=pd.concat([data,file4],axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: we are doing this step by step- there are faster methods, to concat all files at once \n",
    "     \n",
    "     we could use something like:  \n",
    "data = pd.concat([data,file2,file3, file4], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the shape to ensure you have the correct no of rows and columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Standardizing header names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some standards are:\n",
    "    use lower case\n",
    "    if headers have spaces, replace them with underscores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at one column header (using the index position)\n",
    "data.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many columns do we have?\n",
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to make all the columns into lower case \n",
    "cols = []\n",
    "for i in range(len(data.columns)):\n",
    "    cols.append(data.columns[i].lower())\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the columns from our function\n",
    "\n",
    "data.columns = cols\n",
    "\n",
    "#hint: we could also have used a STR function data.columns=data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head() to check the df \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examining the columns and looking for empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data types of all columns \n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at how many NA values we have in each column\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can easily create a df of missing values using isna()\n",
    "\n",
    "missingdata=data.isna()\n",
    "missingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what about showing the % nulls - heres one technique\n",
    "#the above data frame is a boolean ie 1s and 0s - so if we add them up we can see a count of actual values \n",
    "\n",
    "missingdata.sum()/len(data)\n",
    "\n",
    "#what does this look like from maths? \n",
    "#Summing up all the values in a column and then dividing by the total number is the mean.\n",
    "#this is the same as missingdata.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to summarise this in one line of code and round the values \n",
    "data.isna().mean().round(4) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets assume we wanted to drop a single column due to poor coverage across all data sets: domain \n",
    "\n",
    "data = data.drop(['domain'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#what we are left with \n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this time lets drop a few columns in one hit by choosing which we want to keep - also rearranging the columns\n",
    "\n",
    "data=data[['controln', 'state', 'gender', 'hv1', 'hvp1',\n",
    "       'pobc1', 'pobc2','avggift', 'target_d','ic1','ic2','ic3','ic4','ic5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets rename a few of the columns to sensible names\n",
    "data = data.rename(columns={ 'controln':'id','hv1':'median_home_val', 'ic1':'median_household_income'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review the data using head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heres one i wish i had dun earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"face_palm.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - I noticed earlier when running data.info() that I had forgotten to reset the index \n",
    "#after merging all the data frames! oops ... Lets do that now ....\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering and subsetting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#method 1 focus on just male donors in florida \n",
    "\n",
    "data[(data[\"state\"]==\"FL\") & (data[\"gender\"]=='M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#alternative method : \n",
    "\n",
    "data.query('gender==\"M\" & state==\"FL\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last option \n",
    "data.loc[data.gender == \"M\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quick question - are we picking up all males in our data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#challenge1: view a filtered subset of the data where the average gift is over 10 dollars \n",
    "\n",
    "data[(data['avggift']>10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#challenge 2: view a filtered subset of the data where the gender is M, state is florida and avg gift size is more than 10 dollars\n",
    "\n",
    "data.query('gender==\"M\" & state==\"FL\" & avggift > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#before we apply any lasting filter to our data frame lets create a new version which we can play with\n",
    "# so that the original data frame wont be affected\n",
    "\n",
    "tempdata=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use head(), .columns or shape() to review the tempdata\n",
    "\n",
    "tempdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a filtered dataframe from our M gender subset \n",
    "filtered =data[data['gender']=='M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use shape for your filtered df \n",
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head to review the top rows of the filtered df\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice we need to reset the index \n",
    "filtered.reset_index(drop=True, inplace=True)\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the index to display the first ten rows \n",
    "filtered[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the index to display 3 columns, first ten rows \n",
    "filtered[['gender', 'ic2', 'ic3']][23:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the index to display rows with index number 1 and 2 (remember an index starts at 0)\n",
    "filtered[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use loc to return a selected row \n",
    "filtered.loc[326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use iloc to return first 10 rows and the first 4 columns \n",
    "\n",
    "filtered.iloc[1:10,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc v iloc \n",
    "loc is label based\n",
    "iloc is integer based only\n",
    "https://www.analyticsvidhya.com/blog/2020/02/loc-iloc-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sian typo! should refer to id not controln\n",
    "\n",
    "# tip : its possible to set the index from any column eg CONTROLN:id\n",
    "\n",
    "filtered2=filtered.reset_index().set_index('id')\n",
    "filtered2.head()\n",
    "#in which case using iloc and loc would give very different results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when reviewing subsets it is often smart to reconfigure how many rows you will see max\n",
    "\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.loc[:218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sian typo! should say filtered2 not filtered 3\n",
    "\n",
    "filtered.iloc[:218]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning steps (1) data type change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverting back to our temp data copy\n",
    "# lets look at the data types. any data type worth changing? \n",
    "tempdata.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on float data types \n",
    "tempdata.select_dtypes('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple change from float to int type would drop the decimals \n",
    "tempdata['avggift'] = tempdata['avggift'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on the object data types \n",
    "tempdata.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although tempting to force an object into a float using as type :\n",
    "\n",
    "tempdata['median_home_val'] = tempdata['median_home_val'].astype('float', errors='ignore')\n",
    "\n",
    "we know some values of the data are strings and this could produce an error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a look at all the numeric data \n",
    "\n",
    "tempdata._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do a data type change for column and replace non float values with NaN\n",
    "\n",
    "tempdata['median_home_val'] =  pd.to_numeric(tempdata['median_home_val'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for median household income, ic3 and ic5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata['median_household_income'] =  pd.to_numeric(tempdata['median_household_income'], errors='coerce')\n",
    "tempdata['ic3'] =  pd.to_numeric(tempdata['ic3'], errors='coerce')\n",
    "tempdata['ic5'] =  pd.to_numeric(tempdata['ic5'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata.to_csv('tempdataoutput.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning steps (2) duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets rename our tempdata df into cleandata\n",
    "cleandata =tempdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all dupes using df.drop_duplicates()- remember to replace the dataframe with the de duped df\n",
    "\n",
    "#we can also do a de dupe on specified columns such as:\n",
    "#cleandata = cleandata.drop_duplicates(subset=['state','gender', 'ic2', 'ic3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review using shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning steps (3) null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review where are we starting from - use info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a more detailed snapshot, we can calculate the % nulls per column as a new df\n",
    "nulls_df = pd.DataFrame(round(cleandata.isna().sum()/len(cleandata),4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy the nulls_df by resetting the index and renaming the columns as 'header_name' & 'percent_nulls'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical data nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could potentially drop columns we see as having high null % or drop columns according to a null% rule \n",
    "columns_drop = nulls_df[nulls_df['percent_nulls']>3]['header_name']  #3% threshold, arbitrary value\n",
    "print(columns_drop.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example dropping the gender column on a new dataframe cleandata1- just an example, we wont do this with our df: \n",
    "cleandata1 = cleandata.drop(['gender'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many rows would this effect? \n",
    "\n",
    "cleandata[cleandata['gender'].isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative - replace the nulls with a sensible value. one option is use the most common value\n",
    "\n",
    "#step 1 find the most common gender value in the data set with the value counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2 fillna with most freq value using fillna\n",
    "cleandata['gender'] = cleandata['gender'].fillna('F')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the nulls % data frame creation steps again to see what your cleandata data frame looks like now \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for handling null values in categorical variables \n",
    "    \n",
    "# Ignore observation\n",
    "# Replace by most frequent value\n",
    "# Replace using an algorithm like KNN using the neighbours.\n",
    "# Predict the observation using a multiclass predictor.\n",
    "# Treat missing data as just another category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numerical data nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run nulls_df if you need a reminder of how many nulls we have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if some columns contain relatively few null values in our data we can filter the null rows away (drop them). \n",
    "cleandata = cleandata[cleandata['ic2'].isna()==False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for ic4 and ic5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with columns that have more nulls we will use an impute method, for example using the mean value \n",
    "mean_median_home_value = np.mean(cleandata['median_home_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fillna to complete the step\n",
    "cleandata['median_home_val'] = cleandata['median_home_val'].fillna(mean_median_home_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options for null values in Numerical columns: \n",
    "\n",
    "# Ignore these observations\n",
    "# Replace with general average\n",
    "# Replace with similar type of averages\n",
    "# Build model to predict missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning steps (4) too many unique (categorical) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many unique values do we have in the gender column?\n",
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introducing lamba and map to solve our gender issue \n",
    "\n",
    "#A lambda function is a small anonymous function.\n",
    "\n",
    "#A lambda function can take any number of arguments, but can only have one expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some simple maths examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x: x+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what would we get with \n",
    "y(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = lambda x,y : x+y\n",
    "addition(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda x: x*x\n",
    "square(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1,2,3,4,5,6,7,8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []for item in lst:\n",
    "    new_list.append(square(item))\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could also have used list comprehension\n",
    "new_list = [square(item) for item in lst] \n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply lambda from above to square only the even numbers in our lst\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3schools.com/python/python_lambda.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map function \n",
    "# The map() function executes a specified function for each item in an iterable. The item is sent to the function as a parameter.\n",
    "\n",
    "def myfunc(n):\n",
    "  return len(n)\n",
    "\n",
    "x = map(myfunc, ('apple', 'banana', 'cherry'))\n",
    "\n",
    "print(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heres a simple map/lambda combination to make all gender labels upper case \n",
    "cleandata['gender'] = list(map(lambda x: x.upper(), cleandata['gender'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda is good for quick jobs like lower/upper STR functions that you dont need much transparency over \n",
    "#for more complicated tasks, create a function to match the unique values to M or F using IF logic\n",
    "\n",
    "def clean(x):\n",
    "    if x in ['M', 'MALE']:\n",
    "        return 'Male'\n",
    "    elif x.startswith('F'):\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the above function with map to create a smaller set of gender labels in our cleandata \n",
    "#- hint the function clean should be in place of lambda as shown in the last map \n",
    "\n",
    "#syntax structure : df['column']=list(map(functionname, df['column']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use value_counts() or df['column'].unique() to confirm the effect of what you just did\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTION - create buckets to solve too many unique values \n",
    "\n",
    "this depends on the business case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create buckets on the ic2 data based on cutting the data range into 4 sections. \n",
    "ic2_labels = ['Low', 'Moderate', 'High', 'Very High']\n",
    "cleandata['ic2_'] = pd.cut(cleandata['ic2'],4, labels=ic2_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check what the bins look like \n",
    "\n",
    "pd.cut(cleandata['ic2'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively using quantiles to cut the data could be helpful \n",
    "#pd.qcut = “Quantile-based discretization function.” \n",
    "#This basically means that qcut tries to divide up the underlying data into equal sized bins. \n",
    "\n",
    "#The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at another candidate column for this with describe \n",
    "cleandata['ic3'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and with value_counts() can we get an idea of how unique each value is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut up ic3 into 4 quantiles - let Python work out the rest\n",
    "\n",
    "pd.qcut(cleandata['ic3'], q=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A common use case is to store the bin results back in the original dataframe for future analysis. \n",
    "#For this example, we will create 4 bins (aka quartiles) and store the results back in the original dataframe:\n",
    "cleandata['quantile_ic3'] = pd.qcut(cleandata['ic3'], q=4)\n",
    "\n",
    "cleandata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use value_counts on the new quantile ic3 field to see how the split \n",
    "\n",
    "cleandata['quantile_ic3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a clear disadvantage - not super easy to interpret to the end user and requires relabelling at some point. \n",
    "# But for transparency lets leave it as is - for more information on this: https://pbpython.com/pandas-qcut-cut.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finally - lets export what we have so far to a csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting this processed cleandata to a csv\n",
    "cleandata.to_csv('merged_clean_ver1.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
